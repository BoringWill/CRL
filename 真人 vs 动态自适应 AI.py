import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pygame
from collections import deque
from slime_env import SlimeSelfPlayEnv, FrameStack

# --- é…ç½® ---
CONFIG = {
    "model_path": "æ¨¡å‹é›†_opponent/train_20260125-013011/fixed_opponent_current.pth",  # æ›¿æ¢ä¸ºä½ æœ€å¥½çš„æ¨¡å‹
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    # åŠ¨æ€éš¾åº¦å‚æ•°
    "init_temp": 1.0,  # åˆå§‹æ¸©åº¦
    "min_temp": 0.05,  # æœ€ä½æ¸©åº¦ (æ¥è¿‘0ï¼Œä»£è¡¨æœ€å¼º/æœ€è®¤çœŸ)
    "max_temp": 5.0,  # æœ€é«˜æ¸©åº¦ (ä»£è¡¨éå¸¸éšæœº/åœ¨ä¹±ç©)
    "temp_step": 0.05,  # æ¯æ¬¡è°ƒæ•´çš„å¹…åº¦
    "gamma": 0.99,  # æŠ˜æ‰£å› å­ (è™½ç„¶ä½ å…¬å¼æ˜¯ r+V'-Vï¼Œä½†é€šå¸¸VåŒ…å«gammaï¼Œè¿™é‡Œä¿ç•™é€‰é¡¹)
}


# --- 1. æ¨¡å‹ç»“æ„ (å¿…é¡»åŒ…å« Actor å’Œ Critic) ---
class Agent(nn.Module):
    def __init__(self):
        super(Agent, self).__init__()
        self.critic = nn.Sequential(
            nn.Linear(48, 256), nn.ReLU(),
            nn.Linear(256, 128), nn.ReLU(),
            nn.Linear(128, 1)
        )
        self.actor = nn.Sequential(
            nn.Linear(48, 256), nn.ReLU(),
            nn.Linear(256, 128), nn.ReLU(),
            nn.Linear(128, 4)
        )

    def get_value(self, obs, device):
        """è·å–çŠ¶æ€ä»·å€¼ V(s)"""
        with torch.no_grad():
            t_obs = torch.as_tensor(obs, dtype=torch.float32, device=device)
            if t_obs.dim() == 1: t_obs = t_obs.unsqueeze(0)
            return self.critic(t_obs).item()

    def get_action_with_temp(self, obs, temp, device):
        """æ ¹æ®æ¸©åº¦é‡‡æ ·åŠ¨ä½œ"""
        with torch.no_grad():
            t_obs = torch.as_tensor(obs, dtype=torch.float32, device=device)
            if t_obs.dim() == 1: t_obs = t_obs.unsqueeze(0)

            logits = self.actor(t_obs)

            # --- å…³é”®ï¼šåº”ç”¨æ¸©åº¦ç³»æ•° ---
            # æ¸©åº¦è¶Šä½ -> åˆ†å¸ƒè¶Šå°–é” -> è¶Šæ¥è¿‘ argmax (è®¤çœŸ)
            # æ¸©åº¦è¶Šé«˜ -> åˆ†å¸ƒè¶Šå¹³å¦ -> è¶Šæ¥è¿‘å‡åŒ€åˆ†å¸ƒ (éšæœº/æ”¾æ°´)
            if temp < 1e-3:  # é˜²æ­¢é™¤ä»¥0
                action = torch.argmax(logits, dim=1)
            else:
                # Logits é™¤ä»¥æ¸©åº¦
                probs = F.softmax(logits / temp, dim=-1)
                dist = torch.distributions.Categorical(probs)
                action = dist.sample()

            return action.cpu().numpy()[0]


def load_weights(model, path, device):
    if not os.path.exists(path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return False
    try:
        ckpt = torch.load(path, map_location=device)
        sd = ckpt["model_state_dict"] if isinstance(ckpt, dict) and "model_state_dict" in ckpt else ckpt
        model.load_state_dict(sd, strict=False)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {path}")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False


# --- 2. åŠ¨æ€è°ƒæ•´ä¸»å¾ªç¯ ---
def run_adaptive_game():
    pygame.init()
    pygame.font.init()
    # è®¾ç½®å¤§ä¸€ç‚¹çš„å­—ä½“ä»¥ä¾¿è§‚å¯Ÿæ•°æ®
    font = pygame.font.SysFont('Arial', 24, bold=True)

    # åˆå§‹åŒ–ç¯å¢ƒ
    raw_env = SlimeSelfPlayEnv(render_mode="human")
    env = FrameStack(raw_env, n_frames=4)

    # åŠ è½½ AI
    ai_agent = Agent().to(CONFIG["device"])
    if not load_weights(ai_agent, CONFIG["model_path"], CONFIG["device"]):
        return

    ai_agent.eval()

    # åˆå§‹åŒ–å˜é‡
    current_temp = CONFIG["init_temp"]
    running = True
    clock = pygame.time.Clock()

    # é˜Ÿåˆ—åˆå§‹åŒ–
    p1_dq = deque([np.zeros(12) for _ in range(4)], maxlen=4)  # çœŸäºº
    p2_dq = deque([np.zeros(12) for _ in range(4)], maxlen=4)  # AI

    # åˆå§‹é‡ç½®
    obs, _ = env.reset()
    # è¿™é‡Œçš„obsæ˜¯FrameStackåçš„ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ç»´æŠ¤é˜Ÿåˆ—æ¥æ¨¡æ‹Ÿ step
    # ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥åˆ©ç”¨ env å†…éƒ¨çŠ¶æ€æˆ–æ‰‹åŠ¨æ­¥è¿›
    # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨æ ‡å‡†æµç¨‹ï¼Œåˆå§‹æ—¶ obs å·²ç»å †å å¥½äº†

    # åˆ†è§£åˆå§‹ obs (Gym vector env å¯èƒ½ä¼šæœ‰ä¸åŒï¼Œè¿™é‡Œå‡è®¾ standard Box)
    # SlimeSelfPlayEnv çš„ reset è¿”å›çš„æ˜¯ (obs1, obs2) è¿˜æ˜¯å•è¾¹ï¼Ÿ
    # æ ¹æ®ä¹‹å‰çš„ä»£ç ï¼ŒFrameStack é€šå¸¸åŒ…è£…åè¿”å›å•ä¸ª obsã€‚
    # è¿™é‡Œä¸ºäº†ç¡®ä¿é€»è¾‘æ­£ç¡®ï¼Œæˆ‘ä»¬æ‰‹åŠ¨ç»´æŠ¤ obs é˜Ÿåˆ—ï¼ˆå°±åƒä½ ä¹‹å‰çš„è„šæœ¬ä¸€æ ·ï¼‰

    raw_obs_p1 = raw_env._get_obs(1)
    raw_obs_p2 = raw_env._get_obs(2)
    for _ in range(4):
        p1_dq.append(raw_obs_p1)
        p2_dq.append(raw_obs_p2)

    # è®°å½•ä¸Šä¸€å¸§çš„ AI ä»·å€¼ V(s)
    last_val_p2 = 0.0
    td_error = 0.0

    print("\n>>> ğŸ® åŠ¨æ€éš¾åº¦ AI å·²å°±ç»ªï¼")
    print(">>> è§‚å¯Ÿå·¦ä¸Šè§’æ•°æ®ï¼šæ¸©åº¦(Temp)è¶Šé«˜ä»£è¡¨AIè¶Šå‚»ï¼Œè¶Šä½ä»£è¡¨AIè¶Šå¼ºã€‚")

    while running:
        # 1. å¤„ç†äº‹ä»¶
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False

        # 2. è·å–çœŸäººåŠ¨ä½œ
        keys = pygame.key.get_pressed()
        action_p1 = 0
        if keys[pygame.K_a]: action_p1 = 1  # å·¦
        if keys[pygame.K_d]: action_p1 = 2  # å³
        if keys[pygame.K_w]: action_p1 = 3  # è·³

        # 3. å‡†å¤‡ AI è§‚æµ‹æ•°æ® (s)
        obs_p2_stack = np.concatenate(list(p2_dq))

        # 4. è®¡ç®—å½“å‰çŠ¶æ€ä»·å€¼ V(s)
        current_val_p2 = ai_agent.get_value(obs_p2_stack, CONFIG["device"])

        # 5. AI æ ¹æ®å½“å‰æ¸©åº¦è¡ŒåŠ¨
        action_p2 = ai_agent.get_action_with_temp(obs_p2_stack, current_temp, CONFIG["device"])

        # 6. ç¯å¢ƒæ­¥è¿›
        # step è¿”å›: obs, reward, terminated, truncated, info
        # æ³¨æ„: SlimeSelfPlayEnv è¿”å›çš„æ˜¯ä¸¤ä¸ªæ™ºèƒ½ä½“çš„ observation
        # ä½†æˆ‘ä»¬æ‰‹åŠ¨ç»´æŠ¤ dequeï¼Œæ‰€ä»¥åªéœ€è¦ reward å’Œ info
        obs_pair, rewards, term, trunc, info = env.step((action_p1, action_p2))

        # è·å– AI çš„å¥–åŠ± (P2)
        # rewards é€šå¸¸æ˜¯ (rew1, rew2) æˆ–è€…æ ¹æ®ç¯å¢ƒå®šä¹‰
        # å‡è®¾ step è¿”å›çš„æ˜¯ (obs1, obs2), (rew1, rew2)...
        # å¦‚æœ env wrapper æ”¹å˜äº†è¿”å›æ ¼å¼ï¼Œè¿™é‡Œéœ€è¦é€‚é…ã€‚
        # æŒ‰ç…§ SlimeSelfPlayEnv åŸç”Ÿé€»è¾‘ï¼š
        reward_p2 = rewards if isinstance(rewards, (int, float)) else rewards[1]
        # å¦‚æœæ˜¯ self-play wrapperï¼Œé€šå¸¸ reward æ˜¯é’ˆå¯¹ P1 çš„ï¼ŒP2 = -reward
        # è®©æˆ‘ä»¬å‡è®¾ reward æ˜¯é’ˆå¯¹ P1 çš„ï¼š
        if isinstance(rewards, (float, int)):
            # å¦‚æœè¿”å›å•å€¼ï¼Œé€šå¸¸æ˜¯ P1 çš„ reward
            reward_p2 = -rewards
        else:
            reward_p2 = rewards[1]

        # 7. æ›´æ–°è§‚æµ‹é˜Ÿåˆ— -> å¾—åˆ° (s')
        raw_obs_p1_new = raw_env._get_obs(1)
        raw_obs_p2_new = raw_env._get_obs(2)
        p1_dq.append(raw_obs_p1_new)
        p2_dq.append(raw_obs_p2_new)

        obs_p2_next_stack = np.concatenate(list(p2_dq))

        # 8. è®¡ç®—ä¸‹ä¸€çŠ¶æ€ä»·å€¼ V(s')
        # å¦‚æœæ¸¸æˆç»“æŸï¼ŒV(s') åº”å½“ä¸º 0
        if term or trunc:
            next_val_p2 = 0.0
        else:
            next_val_p2 = ai_agent.get_value(obs_p2_next_stack, CONFIG["device"])

        # ==========================================
        # 9. æ ¸å¿ƒé€»è¾‘ï¼šè®¡ç®— TD Error å¹¶è°ƒæ•´æ¸©åº¦
        # å…¬å¼: TD = r + V' - V (å¿½ç•¥ gamma æˆ–è®¾ gamma=1.0 ä»¥ä¸¥æ ¼åŒ¹é…ä½ çš„æè¿°)
        # ==========================================

        # ä¸ºäº†è®©æ•ˆæœæ›´æ˜æ˜¾ï¼Œæˆ‘ä»¬å¯ä»¥ç»™ reward åŠ ä¸€ç‚¹æƒé‡ï¼Œæˆ–è€…ä¿ç•™åŸæ ·
        td_error = reward_p2 + next_val_p2 - current_val_p2

        # --- åŠ¨æ€è°ƒæ•´è§„åˆ™ ---
        if td_error >= 0:
            # å±€é¢æ¯”é¢„æœŸå¥½ (V' > V) æˆ–è€… å¾—åˆ†äº† (r > 0)
            # AI: "ä¼˜åŠ¿åœ¨æˆ‘ï¼Œæˆ‘è¦æµªä¸€ç‚¹" -> æ¸©åº¦å‡é«˜
            current_temp += CONFIG["temp_step"]
        else:
            # å±€é¢æ¯”é¢„æœŸå·® (V' < V) æˆ–è€… ä¸¢åˆ†äº† (r < 0)
            # AI: "æƒ…å†µä¸å¦™ï¼Œæˆ‘è¦è®¤çœŸäº†" -> æ¸©åº¦é™ä½
            current_temp -= CONFIG["temp_step"]

        # é™åˆ¶æ¸©åº¦èŒƒå›´
        current_temp = max(CONFIG["min_temp"], min(CONFIG["max_temp"], current_temp))

        # ==========================================

        # 10. æ¸²æŸ“ç”»é¢ä¸æ•°æ®
        raw_env.render()

        # åœ¨å±å¹•ä¸Šç»˜åˆ¶æ•°æ®
        screen = pygame.display.get_surface()
        if screen:
            # ç»˜åˆ¶èƒŒæ™¯æ¡†
            pygame.draw.rect(screen, (0, 0, 0), (10, 10, 350, 100))

            # 1. æ˜¾ç¤ºæ¸©åº¦ (AI æ™ºå•†çŠ¶æ€)
            if current_temp < 0.2:
                status = "Serious (Try Hard)"
                color = (255, 50, 50)  # çº¢ - è®¤çœŸ
            elif current_temp < 1.5:
                status = "Normal"
                color = (255, 255, 0)  # é»„ - æ­£å¸¸
            else:
                status = "Relaxed (Random)"
                color = (50, 255, 50)  # ç»¿ - ä¼‘é—²

            txt_temp = font.render(f"AI Temp: {current_temp:.2f} | {status}", True, color)
            screen.blit(txt_temp, (20, 20))

            # 2. æ˜¾ç¤º TD Error
            txt_td = font.render(f"TD Error: {td_error:.4f} (r={reward_p2})", True, (200, 200, 255))
            screen.blit(txt_td, (20, 50))

            # 3. æ˜¾ç¤ºä»·å€¼ä¼°è®¡
            txt_val = font.render(f"Value(s): {current_val_p2:.3f}", True, (200, 200, 255))
            screen.blit(txt_val, (20, 80))

            pygame.display.flip()

        clock.tick(60)

        # å±€é—´é‡ç½®
        if term or trunc:
            print(f"æœ¬å±€ç»“æŸ | Temp: {current_temp:.2f}")
            p1_dq = deque([np.zeros(12) for _ in range(4)], maxlen=4)
            p2_dq = deque([np.zeros(12) for _ in range(4)], maxlen=4)
            raw_obs_p1 = raw_env._get_obs(1)
            raw_obs_p2 = raw_env._get_obs(2)
            for _ in range(4):
                p1_dq.append(raw_obs_p1)
                p2_dq.append(raw_obs_p2)
            env.reset()

    pygame.quit()


if __name__ == "__main__":
    run_adaptive_game()