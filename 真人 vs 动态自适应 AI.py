import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import deque
from slime_env import SlimeSelfPlayEnv
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import Manager  # å¯¼å…¥å…±äº«ç®¡ç†å™¨

# --- æ ¸å¿ƒé…ç½® ---
CONFIG = {
    "adaptive_model_path": "æœ€å¼ºæ¨¡å‹é›†/1.pth",
    "opponents_dir": "æœ€å¼ºæ¨¡å‹é›†",
    "device": torch.device("cpu"),  # å¤šè¿›ç¨‹å¹¶å‘å»ºè®®ç”¨ CPUï¼Œå¦åˆ™æ˜¾å­˜å®¹æ˜“çˆ†
    "init_temp": 1.0,
    "min_temp": 0.1,
    "max_temp": 10.0,
    "inertia": 0.5,
    "ema_alpha": 0.01,
    "max_workers": 10,  # åŒæ—¶å¯åŠ¨çš„å¹¶è¡Œè¿›ç¨‹æ•° (æ ¹æ®ä½ çš„ CPU æ ¸å¿ƒæ•°è°ƒæ•´)
}


# --- 1. é€»è¾‘ç»„ä»¶ (ä¿æŒä¸å˜) ---
class SmartDifficultyManager:
    def __init__(self, init_min=-0.1, init_max=0.1, init_smooth=0.5):
        self.min_v = init_min
        self.max_v = init_max
        self.smooth_confidence = init_smooth
        self.current_temp = CONFIG["init_temp"]

    def update(self, v_raw, p1_score, p2_score):
        self.min_v = min(self.min_v, v_raw)
        self.max_v = max(self.max_v, v_raw)
        range_v = self.max_v - self.min_v
        instant_conf = (v_raw - self.min_v) / range_v if range_v > 1e-5 else 0.5
        self.smooth_confidence = self.smooth_confidence * (1 - CONFIG["ema_alpha"]) + instant_conf * CONFIG["ema_alpha"]
        target_temp = 0.1 + (self.smooth_confidence * 9.9)
        score_diff = p1_score - p2_score
        if score_diff > 0:
            target_temp = max(CONFIG["min_temp"], target_temp - (score_diff * 1.5))
        diff = target_temp - self.current_temp
        move_speed = CONFIG["inertia"] * (0.5 if self.current_temp < 1.0 else 1.0)
        self.current_temp += diff * move_speed
        self.current_temp = np.clip(self.current_temp, CONFIG["min_temp"], CONFIG["max_temp"])
        return self.current_temp, self.smooth_confidence


class Agent(nn.Module):
    def __init__(self):
        super(Agent, self).__init__()
        self.critic = nn.Sequential(nn.Linear(52, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 1))
        self.actor = nn.Sequential(nn.Linear(52, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 4))

    def get_value(self, obs):
        with torch.no_grad():
            t_obs = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0)
            return self.critic(t_obs).item()

    def get_action(self, obs, temp):
        with torch.no_grad():
            t_obs = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0)
            logits = self.actor(t_obs)
            probs = F.softmax(logits / temp, dim=-1)
            return torch.distributions.Categorical(probs).sample().item()


def load_weights(model, path):
    if not os.path.exists(path): return False
    ckpt = torch.load(path, map_location="cpu")
    model.load_state_dict(ckpt["model_state_dict"] if "model_state_dict" in ckpt else ckpt, strict=True)
    return True


# --- 2. å•åœºå¯¹æˆ˜ä»»åŠ¡ ---
def play_one_match(opp_name, current_state_snapshot, render=False):
    # åˆå§‹åŒ–ç¯å¢ƒ
    render_mode = "human" if render else None
    env = SlimeSelfPlayEnv(render_mode=render_mode)

    # åˆå§‹åŒ–æ¨¡å‹
    agent_adaptive = Agent()
    load_weights(agent_adaptive, CONFIG["adaptive_model_path"])

    agent_fixed = Agent()
    opp_path = os.path.join(CONFIG["opponents_dir"], opp_name)
    load_weights(agent_fixed, opp_path)

    # ä½¿ç”¨ä¼ å…¥çš„å¿«ç…§åˆå§‹åŒ–ï¼ˆç»§æ‰¿å€¼ï¼‰
    diff_manager = SmartDifficultyManager(
        init_min=current_state_snapshot['min_v'],
        init_max=current_state_snapshot['max_v'],
        init_smooth=current_state_snapshot['smooth_confidence']
    )

    p1_dq, p2_dq = deque(maxlen=4), deque(maxlen=4)
    raw_obs_p1, _ = env.reset();
    raw_obs_p2 = env._get_obs(2)
    for _ in range(4): p1_dq.append(raw_obs_p1); p2_dq.append(raw_obs_p2)

    done = False
    while not done:
        curr_temp = diff_manager.current_temp
        obs_p1 = np.concatenate(list(p1_dq))
        obs_p2 = np.concatenate(list(p2_dq))

        a1 = agent_fixed.get_action(obs_p1, 0.01)
        a2 = agent_adaptive.get_action(obs_p2, curr_temp)

        n_p1, n_p2, _, term, trunc, _ = env.step((a1, a2))
        p1_dq.append(n_p1);
        p2_dq.append(n_p2)

        v_next = 0.0 if (term or trunc) else agent_adaptive.get_value(np.concatenate(list(p2_dq)))
        diff_manager.update(v_next, env.p1_score, env.p2_score)

        if term or trunc:
            done = True

    result = {
        "opponent": opp_name,
        "p1_score": env.p1_score,
        "p2_score": env.p2_score,
        "win": env.p2_score > env.p1_score,
        "final_state": {
            "min_v": diff_manager.min_v,
            "max_v": diff_manager.max_v,
            "smooth_confidence": diff_manager.smooth_confidence
        }
    }
    return result


# --- 3. å¤šçº¿ç¨‹æ§åˆ¶å™¨ ---
def run_fast_tournament():
    opp_files = [f for f in os.listdir(CONFIG["opponents_dir"]) if f.endswith(".pth")]
    opp_files.sort()

    print(f"ğŸš€ å¼€å§‹å¹¶å‘èµ›æ¨¡å¼ | å¹¶è¡Œæ•°: {CONFIG['max_workers']} | å¯¹æ‰‹æ€»æ•°: {len(opp_files)}")
    print("=" * 60)

    all_results = []

    # å»ºç«‹è¿›ç¨‹é—´å…±äº«çš„çŠ¶æ€å­—å…¸
    with Manager() as manager:
        shared_state = manager.dict({"min_v": -0.1, "max_v": 0.1, "smooth_confidence": 0.5})

        with ProcessPoolExecutor(max_workers=CONFIG["max_workers"]) as executor:
            # æäº¤ä»»åŠ¡æ—¶ï¼Œä¼ å…¥ shared_state çš„å½“å‰æ‹·è´
            futures = {executor.submit(play_one_match, name, dict(shared_state), False): name for name in opp_files}

            for future in as_completed(futures):
                res = future.result()
                # å…³é”®ï¼šä¸€ä¸ªä»»åŠ¡ç»“æŸï¼Œç«‹å³æ›´æ–°å…±äº«å­—å…¸ï¼Œä¾›åç»­è¿˜æ²¡å¼€å§‹çš„ä»»åŠ¡è¯»å–
                shared_state.update(res["final_state"])

                all_results.append(res)
                status = "ğŸ† WIN" if res['win'] else "âŒ LOSS"
                print(f"[{status}] {res['opponent'].ljust(25)} | P1: {res['p1_score']} vs P2: {res['p2_score']}")

    # æ‰“å°æœ€ç»ˆæˆ˜ç»©æ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆæˆ˜ç»©ç»Ÿè®¡")
    print("-" * 60)
    wins = sum(1 for r in all_results if r['win'])
    total = len(all_results)
    print(f"æ€»åœºæ¬¡: {total} | èƒœç‡: {wins / total:.2%} | èƒœ: {wins} / è´Ÿ: {total - wins}")
    print("=" * 60)


if __name__ == "__main__":
    mode = input("1. å¿«é€Ÿæ¨¡å¼ (å¹¶å‘+æ— æ¸²æŸ“)\n2. è§‚æˆ˜æ¨¡å¼ (å•çº¿ç¨‹+æœ‰æ¸²æŸ“)\nè¯·é€‰æ‹©: ")

    if mode == "1":
        run_fast_tournament()
    else:
        opp_files = [f for f in os.listdir(CONFIG["opponents_dir"]) if f.endswith(".pth")]
        opp_files.sort()
        # è§‚æˆ˜æ¨¡å¼ä¹Ÿæ‰‹åŠ¨ç»´æŠ¤è¿™ä¸ªç»§æ‰¿çŠ¶æ€
        current_state = {"min_v": -0.1, "max_v": 0.1, "smooth_confidence": 0.5}
        for f in opp_files:
            res = play_one_match(f, current_state, render=True)
            current_state = res["final_state"]